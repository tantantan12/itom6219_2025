{"cells":[{"cell_type":"markdown","metadata":{"id":"6KYjOyhx8kGB"},"source":["# Week 3 - Python and Natural Language Processing"]},{"cell_type":"markdown","source":["# This is my first Python practice."],"metadata":{"id":"uP7D4ROaFpoK"}},{"cell_type":"markdown","source":["Install packages and import them."],"metadata":{"id":"26ewPYn7T5WK"}},{"cell_type":"code","source":["!pip install pingouin\n","!pip install --upgrade --force-reinstall git+https://github.com/tantantan12/itom6219.git\n","import pandas as pd\n","import numpy as np\n","from scipy.sparse import csr_matrix\n","from sklearn.decomposition import NMF\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import os"],"metadata":{"id":"KIgBc0ApOmO8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hiJYqSDH8kGC"},"source":["## 1. Introduction to Python\n","To perform data collection and natural language processing, we will learn some basics of Python.  \n","### 1.1 Introduction to Python\n","Python was created in 1990 by Guido van Rossu as a general-purpose and high-level programming language. It has become extremely popular over the past decade because of its intuitive nature, flexibility, and versatility. According to the Developer Nation's recent 30,000 developer survey, Python is among the top three programming language choices of 2023. Python was rated the most popular in data science, machine learning, and artificial intelligence. I hope that this class can show you the charm of Python and motivate you to continue the learning of this great programming language and its associated libraries for data science and machine learning.\n","#### 1.1.1 Google Colab\n","\n","\n","Google Colab, or Google Colaboratory, is a cloud-based platform for writing and running Jupyter notebooks using Google’s cloud resources.\n","\n","\n","- Step 1: Copy the GitHub Repository Link\n","Start by copying the link to the GitHub repository containing the notebook you want to run. Visit the GitHub repository, click the green “Code” button, and select “Copy” to copy the repository link.\n","\n","- Step 2: Access Google Colab\n","Open Google Colab in your web browser at colab.research.google.com. Sign in to your Google account if you’re not already logged in.\n","\n","- Step 3: Connect to the GitHub Repository\n","In the Colab interface, click “GitHub,” enter the GitHub URL, and hit the search icon.\n","\n","- Step 4: Open the Notebook\n","Select the repository, the branch and the Jupyter notebook you want to open.\n","\n","- Step 5: Run the Notebook\n","With the notebook directory open, you can run the notebook cells just as you would in a local Jupyter notebook."]},{"cell_type":"markdown","metadata":{"id":"UUo5o0Nt8kGD"},"source":["#### 1.1.2 Hello World!\n","\n","- Run the \"hello world\" program\n","- Check the version of your Python\n","- Adding comments to your code\n","- Identify an error and fix it\n","\n","<code>print() </code>is a function that takes the istring <code>\"Hello, world!\"</code> as an argument."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7qPkjNwe8kGD"},"outputs":[],"source":["print(\"Hello, class!\")"]},{"cell_type":"markdown","metadata":{"id":"uW6WBjXR8kGE"},"source":["It's a good idea to add comments to your code. These commends not only help you to remember what you did but also help others to understand your steps and objectives. To write comments, use the symbole <code>#</code> before the commend. When running the code, everythng past <code>#</code> will be ignored by the runtime.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JvfVQGb58kGE"},"outputs":[],"source":["print(\"Hello, world!\") # the function print() will print the string it takes as input/argument."]},{"cell_type":"markdown","metadata":{"id":"OwdDRoaJ8kGE"},"source":["Many times, you will encounter errors when you use Python. Please take your time to read the error message before editing your code. Feel free to use ChatGPT to help you understand the error message."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Opuprsz28kGE"},"outputs":[],"source":["print(\"Hello, world!\")"]},{"cell_type":"markdown","metadata":{"id":"hwYChTem8kGE"},"source":["### 1.2 Variables/Data structure\n","Let's try to create a variable x. The equality sign means \"assignment\". Variable names can be anything as long as:\n","- it contains only letters, numbers, or underscores.\n","- the first character is not a number.\n","- the variable name is not one of the reserved keywords.\n","\n","While the variable names can be of anything, it is recommended that you use either of these:\n","- camel case (e.g., variableName)\n","- snake case (e.g., variable_name)"]},{"cell_type":"markdown","metadata":{"id":"s0JeGX_v8kGF"},"source":["### 1.3 Data Type\n","The primary data types within Pyton are integers, floats, boolean, and strings. They can be further stored in Python as lists and dictionaries.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZPV3QlSS8kGF"},"outputs":[],"source":["x=   #integer\n","y=   #float\n","url =   #string\n","ascending=\n"," #An object of type Boolean takes one of the two values: True or False.\n"," #Please make sure to use uppercase \"T\" for True and uppercase \"F\" for False."]},{"cell_type":"code","source":["type(url)"],"metadata":{"id":"AGWbAuJvIZsC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-fkcSGE88kGF"},"source":["#### We can convert numbers to strings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fOtXBXO68kGF"},"outputs":[],"source":["twitterAuthorID=123456\n","str(twitterAuthorID)"]},{"cell_type":"markdown","metadata":{"id":"yedOYz7y8kGF"},"source":["You can use the built-in function <code>type()</code> to check data type. Integers are refered to as <code>int</code>, floats as <code>float</code>, boolean as <code>bool</code>, and character string as <code>str</code>."]},{"cell_type":"markdown","metadata":{"id":"9f__gpWx8kGF"},"source":["- what is the type of twitterAuthorID?\n","- what is the type for the expresssion \"twitterAuthorID1==twitterAuthorID2\"?\n","```python\n","twitterAuthorID1=123456\n","twitterAuthorID2=654321\n","twitterAuthorID1==twitterAuthorID2\n","```\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xamgo0BW8kGF"},"outputs":[],"source":["# write down your answers"]},{"cell_type":"markdown","source":["```f-string``` allows a placeholder for a text.\n","These example would help you understand how to use it.  \n"],"metadata":{"id":"f0pedp5BUG1B"}},{"cell_type":"code","source":["fname = \"John\"\n","age = 36\n","txt1 = \"My name is {fname}, I'm {age}\".format(fname = \"John\", age = 36)\n","txt2 = \"My name is {0}, I'm {1}\".format(\"John\",36)\n","txt3 = \"My name is {}, I'm {}\".format(\"John\",36)\n","txt4 = f\"My name is {fname}, I'm {age}\"  # Most popular"],"metadata":{"id":"01hukhy2UMIj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TiLB79YT8kGG"},"source":["### 1.4 Lists, Matrix, and Dataframe\n","The way data is stored is called its structure.   \n","#### 1.4.1 List\n","Lists are ordered collections that can contain items of different data types and structures. You can store various elements in a single list, such as numbers, strings, and even other lists or dictionaries. This flexibility allows you to organize data in a way that suits your needs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pnq3TQZp8kGG"},"outputs":[],"source":["list_mixed_type = [1, \"Hello\", 3.14, True]\n","#lists with all elements of the same data type.\n","Doc =[] # an empty list\n","Document1= \"It is going to rain today.\"\n","Document2= \"Today I am not going outside.\"\n","Document3= \"I am going to watch the season premiere.\"\n","\n","Doc = [Document1 ,\n"," Document2 ,\n"," Document3]\n","\n","myDoc= Doc[0] #Python is a zero-based language.\n","\n","print(myDoc)\n"]},{"cell_type":"code","source":["Doc.append(\"Are you okay?\")\n","\n","print(Doc)\n"],"metadata":{"id":"iObGURRDKxxF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Doc.remove(\"Are\")\n","\n","print(Doc)"],"metadata":{"id":"BYb4i7JxLOuG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DctQng8b8kGG"},"source":["\n","#### List slicing syntax:\n","<code>list[start:stop:step]</code>\n","\n"," You only need 3 parameters when you want to include a step (like reversing with -1). Otherwise, just start:stop is fine.\n","\n"," | Form      | Meaning                        |\n","|-----------|--------------------------------|\n","| `[:]`     | Everything                     |\n","| `[2:]`    | From index 2 to end            |\n","| `[-3:]`   | Last 3 elements                |\n","| `[::-1]`  | Entire list, reversed          |\n","| `[1:5:2]` | From index 1 to 4, step by 2   |\n","\n","> Example:\n","The list of topic importance, [0.81, 0, 0.81, 0, 1.23, 0], represents the importance of words in a topic ['outside' ,'premiere', 'rain', 'season' ,'today' ,'watch']. You want to find the top 3 words, i.e., the 3 highest values."]},{"cell_type":"code","source":["import numpy as np\n","topic= ['outside' ,'premiere', 'rain', 'season' ,'today' ,'watch']\n","# This array might represent the importance of words in a topic.\n","topic_importance = [0.81, 0, 0.81, 0, 1.23, 0]\n","\n","# Get the indices that would sort the array from smallest to largest\n","sorted_indices = np.array(topic_importance).argsort()  # Only the indices are returned\n","\n","print(\"Sorted indices:\", sorted_indices)"],"metadata":{"id":"fgF5Q1l0Nk_Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oI9CMWvx8kGG"},"outputs":[],"source":["\n","\n","# Get the indices of the top 3 values\n","top_indices =   # List slicing: last 3 indices\n","\n","# Reverse the order to get from largest to smallest\n","top_indices =\n","\n","print(\"Top 3 indices (from largest to smallest):\", top_indices)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MdS9EnKU8kGG"},"outputs":[],"source":["# Practice: Show me the code to find the top 2 words."]},{"cell_type":"markdown","source":["Did you notice that we used the command ```import numpy as np``` in the code? This is the very first time for us to import a package. Packages are an essential building block in programming. Without packages, we’d spend lots of time writing code that’s already been written. Finding and using the right package is key to effectively completing your task."],"metadata":{"id":"qDUn9ZjOVQU6"}},{"cell_type":"markdown","metadata":{"id":"1gI5d9f-8kGG"},"source":["\n","\n","#### 1.4.3 Matrix\n","Matrices are two-dimensional arrays of the same data type, and are commonly used to store structured data such as numbers, word frequencies, or pixel values in images. In Python, matrices can be categorized into two main types:\n","\n","- Dense Matrix: A matrix where most of the values are non-zero.\n","\n","- Sparse Matrix: A matrix where many of the values are zero. Storing such matrices as dense structures can waste memory, so special formats are used to save only the non-zero values and their positions.\n","\n","Matrices in Python are commonly created and manipulated using **NumPy** and **scipy**, two powerful packages for numerical computing.  Did you notice that we used the command ```import numpy as np``` in the code? This is the very first time for us to import a package. Packages are an essential building block in programming. Without packages, we’d spend lots of time writing code that’s already been written. Finding and using the right package is key to effectively completing your task.\n","\n","\n","\n","Below we show the creation of a dense matrix and its corresponding sparse matrix."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_PY3dn2p8kGH"},"outputs":[],"source":["import numpy as np\n","\n","# Create a dense 3x6 matrix filled with zeros\n","dense_matrix = np.zeros((3, 6))\n","\n","# Fill in the non-zero values\n","dense_matrix[0, 2] = 0.795\n","dense_matrix[0, 4] = 0.605\n","\n","dense_matrix[1, 0] = 0.795\n","dense_matrix[1, 4] = 0.605\n","\n","dense_matrix[2, 1] = 0.577\n","dense_matrix[2, 3] = 0.577\n","dense_matrix[2, 5] = 0.577\n","\n","print(\"Dense matrix:\\n\", dense_matrix)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QUiOirar8kGH"},"outputs":[],"source":["from scipy.sparse import csr_matrix\n","\n","# Create the sparse matrix using the dense one\n","sparse_matrix = csr_matrix(dense_matrix)\n","\n","print(\"Sparse matrix:\\n\", sparse_matrix)"]},{"cell_type":"markdown","metadata":{"id":"pPieKLx28kGH"},"source":["\n","#### 1.4.4 Dataframe\n","A Pandas DataFrame is a 2 dimensional data structure, like a table with rows and columns. **Pandas** is an open-source software library built on top of Python specifically for data manipulation and analysis. Pandas offers data structure and operations for  powerful, flexible, and easy-to-use data analysis and manipulation. Pandas strengthens Python by giving the popular programming language the capability to work with **spreadsheet-like data** enabling fast loading, aligning, manipulating, and merging, in addition to other key functions.\n"]},{"cell_type":"markdown","source":["##### Create a Dataframe\n","**Create a dataframe from a matrix**"],"metadata":{"id":"pzAHEIdD_9rU"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jTh5tYaY8kGH"},"outputs":[],"source":["import pandas as pd\n","\n","columnNames=['outside' ,'premiere', 'rain', 'season' ,'today' ,'watch']\n","\n","# Use function pd.DataFrame.sparse.from_spmatrix to create a dataframe from a sparse matrix.\n","df1= pd.DataFrame.sparse.from_spmatrix(sparse_matrix,columns=columnNames)\n","print(df1)\n","\n","# use function pd.DataFrame to create a dataframe from a dense matrix.\n","df2= pd.DataFrame(dense_matrix,columns=columnNames)\n","print(df2)\n"]},{"cell_type":"markdown","source":["**Examine the Dataframe**"],"metadata":{"id":"XbhQJn6XALgl"}},{"cell_type":"code","source":["# head() prints the first five rows of your data, but you can indicate however many rows\n","df1.head()\n","\n","#first two rows\n","df1.head(2)\n","\n","# columns names\n","print(df1.columns)\n","\n","# shape of a dataframe\n","rows,columns=df1.shape\n","\n","print( )\n","print( )\n","\n","# one column\n","print(df1.outside)\n","\n","# more than one column\n","print(df1[[\"outside\",\"premiere\"]])\n","\n","#index one row\n","print(df1.iloc[0])\n","\n","\n","#subset\n","df1[df1.today==0]"],"metadata":{"id":"WU3N5IPIAT6U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Create a dataframe from a csv file**"],"metadata":{"id":"JlFnpoAWGNmY"}},{"cell_type":"code","source":["import pandas as pd\n","# Read the data\n","df = pd.read_csv('AI_tweets_2025.csv')\n","\n","df"],"metadata":{"id":"E_AuPMD0SVwe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","# Summarize\n","df.describe()\n","\n","# Sort\n","df=df.sort_values(by='public_metrics.impression_count', ascending=False)\n","print(df.iloc[0])\n","\n","# convert datetime\n","df['datetime']=pd.to_datetime(df['created_at'])\n","\n","# How to list all the column names?\n","### write your code here\n","\n","# How to list the column of \"text\"?\n","### write your code here\n","\n","# How to list the column of \"text\", \"datetime\", and \"username\"?\n","### write your code here\n","\n","# Which text corresponds to the highest like count?\n","### write your code here\n","\n","# Which text corresponds to the highest reply count?\n","### write your code here"],"metadata":{"id":"3tDIdmrdGmWn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"noelz0nk8kGH"},"source":["\n","### 1.5 Conditional statements & Control flow statement\n","\n","As we write programs, we will need to carry out specific actions based on certain conditions (==, <, <=, >, >=, !=). Conditional statements are used to evaluate whether these certain conditions are being met.\n","\n","The comparison operators can be combined with different logical operators (and, or, not)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r5-E-pqf8kGH"},"outputs":[],"source":["#if statement\n","\n","A=10\n","B=20\n","C = 30\n","\n","if A<B:\n","    print(\"A is less than B\")\n","else:\n","    print(\"B is less than A\")\n","\n","\n","# 'and' logical operator\n","if A < B and C > B:\n","    print(\"Both conditions are true: A is less than B, and C is greater than B.\")\n","\n","# Using 'or' logical operator\n","if A < B or C < B:\n","    print(\"At least one condition is true: A is less than B or C is less than B.\")\n","\n","# Using 'not' logical operator\n","if not A > B:\n","    print(\"The condition 'A > B' is not true (i.e., A is not greater than B).\")"]},{"cell_type":"markdown","metadata":{"id":"CBzZjiVI8kGI"},"source":["Below I show loops for lists:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VHltUBJL8kGI"},"outputs":[],"source":["#for loops for a list\n","columnNames=['outside' ,'premiere', 'rain', 'season' ,'today' ,'watch']\n","top_words=[]\n","top_indices = [4 ,2 ,0]\n","\n","\n","for i in top_indices:\n","        top_words.append(columnNames[i])\n","top_words"]},{"cell_type":"markdown","metadata":{"id":"Y0Nrq2AJ8kGI"},"source":["\n","### 1.6 Functions\n","Functions are prewritten blocks of code that can be invoked to carry out a certain set of actions. For example, print() is a function. You can call functions in multiple ways.\n","- The most intuitive way is to use the function name, followed by parentheses.\n","- Another way is to use \"dot notation\" by placing a period before the name of the function and after a specific object. For example,\n","```\n","target_object.function_name()\n","```\n","- Sometimes, we need to provide the function with certain variables or data values. They are called \"parameters\" or \"arguments\". They are passed to the function by putting them within a set of parentheses that follows the function name. For example,"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LeOnyDdP8kGI"},"outputs":[],"source":["\n","from sklearn.decomposition import NMF\n","\n","nmf_model = NMF(n_components=2, random_state=0)\n","\n","df.to_csv('df.csv', index=False)\n"]},{"cell_type":"markdown","metadata":{"id":"uxNTKsNl8kGI"},"source":["## 2. Natural Language Processing - A Naive Example\n","\n","Before diving into real Twitter data, let’s start with a simple example.\n","Here’s a small corpus consisting of three short documents:\n","\n","- Document 1: It is going to rain today.\n","- Document 2: Today I am not going outside.\n","- Document 3: I am going to watch the season premiere.\n"]},{"cell_type":"code","source":["import math\n","math.log(3)"],"metadata":{"id":"sf4QIZrCXTB7"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7LWBCFfA8kGI"},"outputs":[],"source":["Document1= \"It is going to rain today.\"\n","Document2= \"Today I am not going outside.\"\n","Document3= \"I am going to watch the season premiere.\"\n","Doc = [Document1 ,\n"," Document2 ,\n"," Document3]\n","print(Doc)"]},{"cell_type":"markdown","metadata":{"id":"CWBpjHHi8kGI"},"source":["\n","From this example, we’ll learn how to convert raw text into numerical features — or what we might call columns of numbers. This process is often referred to as vectorization.\n","\n","Once we represent text as vectors, we unlock the ability to perform various types of analysis, including:\n","- Summarization\n","- Clustering\n","- Topic modeling\n","- Information retrieval (e.g., finding similar texts)\n","- Predictive modeling\n","\n","The core idea in Natural Language Processing (NLP) is transforming unstructured text into structured numerical form. While there are many ways to do this, we’ll focus on one of the most widely used and interpretable methods: TF-IDF (Term Frequency–Inverse Document Frequency).\n","\n","TF-IDF is useful in many NLP applications. For example:\n","- Search engines use it to rank the relevance of a document to a search query.\n","- It’s also used in text classification, summarization, and topic modeling.\n","\n","After learning TF-IDF, we’ll apply it in a downstream task — topic modeling — to uncover hidden themes across the documents.\n","\n","While we won’t cover every vectorization technique or downstream task, this example will give you a strong foundation for understanding how an NLP pipeline works.\n","\n","\n","### 2.1 Vectorization: Term Frequency(TF) — Inverse Document Frequency(IDF) Vectorization\n","A corpus can be defined as a collection of documents. In our example, each sentence is a document, and they collectively form a corpus.  \n","\n","To vectorize text data, we use a TF-IDF method.\n","- We first tokenize the text, and then assign an importance score for every term.\n","- The importance score of a term is high when it occurs a lot in a given document and rarely in others.\n","- In short, commonality within a document measured by TF is balanced by rarity between documents measured by IDF. The resulting TF-IDF score reflects the importance of a term for a document in the corpus.\n"]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","vectorizer = TfidfVectorizer() #TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n","analyze = vectorizer.build_analyzer()\n","print(\"Document 1\",analyze(Document1))\n","print(\"Document 2\",analyze(Document2))\n","print(\"Document 3\",analyze(Document3))\n"],"metadata":{"id":"P-x3AbutZFkJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NZukhQe78kGI"},"outputs":[],"source":["\n","X = vectorizer.fit_transform(Doc)\n","\n","print(X)\n","df = pd.DataFrame.sparse.from_spmatrix(X,columns=vectorizer.get_feature_names_out())\n","\n","df"]},{"cell_type":"markdown","metadata":{"id":"skdHrf7E8kGJ"},"source":["\n","We tokenize and generate a vocab of the document. For each document, we can find the TF= (Number of repetitions of word in a document) / (# of words in a document). We can further find the IDF=Log[(Number of documents) / (Number of documents containing the word)]\n","\n","| words      | Doc1 | Doc2| Doc3|IDF Value|\n","| ----------- | ----------- |----------- |----------- |----------- |\n","| going      | 0.16     |0.16|0.12|0|\n","| to   | 0.16       |0|0.12|0.41|\n","|today|0.16|0.16|0|0.41|\n","|i|0|0.16|0.12|0.41|\n","|am|0|0.16|0.12|0.41|\n","|it|0.16|0|0|1.09|\n","|is |0.16|0|0|1.09|\n","|rain|0.16|0|0|1.09|\n","\n","We then construct a document-term matrix using the TF-IDF scores:\n","\n","| Docs      | going |to|today|i|am|it|is|rain|\n","| ------ |------ |------ |------ |------ |------ |------ |------ |------ |\n","| Doc1      | 0  |0.07|0.07|0|0|0.17|0.17|0.17|0.17|\n","| Doc2   | 0  |0|0.07|0.07|0.07|0|0|0|\n","|Doc3|0|0.05|0|0.05|0.05|0|0|0|\n","\n","It is easy to see that 'it', 'is', and 'rain' are important for Doc 1 but not Doc 2 or Doc 3. Each row of the document-term matrix can be thought of as a numeric representation of the documents, which we often term vectors. These numeric representations help you to find similarities between documents.\n","\n","> You might have noticed that stop words such as “to” and “is” are included above. These are usually filtered out in real-world NLP tasks because they don’t carry much meaning.\n","\n","To perform vectorization in Python, we use the <code>TfidfVectorizer</code> from the <code>sklearn</code> package.\n","\n","The steps are:\n","- Create the vectorizer.\n","- Fit it on your corpus.\n","- Transform your corpus into vectors.\n","\n","The function **TfidfVectorizer** takes two parameters.\n","- max_df is used for removing terms that appear too frequently, also known as \"corpus-specific stop words\". For example:\n","    - max_df = 0.50 means \"ignore terms that appear in more than 50% of the documents\".\n","    - max_df = 25 means \"ignore terms that appear in more than 25 documents\".\n","    - The default max_df is 1.0, which means \"ignore terms that appear in more than 100% of the documents\". Thus, the default setting does not ignore any terms.\n","- min_df is used for removing terms that appear too infrequently. For example:\n","    - min_df = 0.01 means \"ignore terms that appear in less than 1% of the documents\".\n","    - min_df = 5 means \"ignore terms that appear in less than 5 documents\".\n","    - The default min_df is 1, which means \"ignore terms that appear in less than 1 document\". Thus, the default setting does not ignore any terms.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fg9DXxlh8kGJ"},"outputs":[],"source":["docs=Doc\n","#Convert a collection of raw documents to a matrix of TF-IDF features.\n","tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=0.1, stop_words='english')\n","tfidf = tfidf_vectorizer.fit_transform(docs)\n","\n","\n","tfidf_df = pd.DataFrame.sparse.from_spmatrix(tfidf, columns=tfidf_vectorizer.get_feature_names_out())\n","tfidf_df"]},{"cell_type":"markdown","metadata":{"id":"1MDM6_GA8kGK"},"source":["Now, after removing stop words, the resulting matrix looks like this (we'll call it M):\n","\n","|     | outside   | premiere | rain     | season   | today    | watch    |\n","|-----|-----------|----------|----------|----------|----------|----------|\n","| 0   | 0         | 0        | 0.795961 | 0        | 0.605349 | 0        |\n","| 1   | 0.795961  | 0        | 0        | 0        | 0.605349 | 0        |\n","| 2   | 0         | 0.57735  | 0        | 0.57735  | 0        | 0.57735  |\n","\n","\n","It is notable that <code>tfidf</code> is a sparse matrix. If you'd like to view it as a full DataFrame, use:"]},{"cell_type":"markdown","metadata":{"id":"-xwatCnk8kGS"},"source":["\n","### 2.2 Non-negative Matrix Factorization (NMF)\n","\n","TF-IDF vectors are great, but high-dimensional. When we have hundreds or thousands of terms, interpretation becomes difficult.\n","\n","To reduce this complexity and uncover latent themes, we use Non-negative Matrix Factorization (NMF), a powerful technique for **topic modeling**.\n","\n"," If we think of the document-term matrix $M$ as a $m \\times n$ matrix with $m$ documents and $n$ terms, $M$ can be factorized as\n","\n","\n","\n","\n","$$\n","M=W \\times H\n","$$\n","\n","- M: Original document-term matrix (e.g., m docs × n terms)\n","- W: Document-topic matrix (m docs × k topics)\n","- H: Topic-term matrix (k topics × n terms)\n","- k: Number of topics\n","\n","NMF finds W and H such that their product approximates M, and all values remain non-negative.\n","\n","This technique helps extract topics from text — where each topic is a combination of words, and each document can belong to multiple topics with different strengths.\n","\n","\n","\n","The function NMF takes two parameters.\n","- n_components is the number of topics\n","- random_state controls the random number generator used in the attribute combining process."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uU3P1RY58kGS"},"outputs":[],"source":["from sklearn.decomposition import NMF\n","\n","nmf_model = NMF(n_components=2, random_state=0)\n","#nmf_model.fit(tfidf)\n","W = nmf_model.fit_transform(tfidf)  # Document-topic matrix\n","feature_names = tfidf_vectorizer.get_feature_names_out()\n","# Display topics\n","from sklearn.decomposition import NMF\n","\n","nmf_model = NMF(n_components=2, random_state=0)\n","#nmf_model.fit(tfidf)\n","W = nmf_model.fit_transform(tfidf)  # Document-topic matrix\n","feature_names = tfidf_vectorizer.get_feature_names_out()\n","topic_names=[]\n","# Assume nmf_model and feature_names are already defined\n","\n","# Loop through each topic\n","for topic_index in range(len(nmf_model.components_)):\n","    topic = nmf_model.components_[topic_index]\n","    print(topic)\n","    # Get the indices of the top 3 words (largest values in the topic)\n","    sorted_indices = topic.argsort()  # sorts from smallest to largest\n","\n","    print(sorted_indices)\n","    top_indices = sorted_indices[-2:]  # get the last 3 (top 3 words)\n","\n","    # Reverse to make it largest to smallest\n","    top_indices = top_indices[::-1]\n","\n","    # Get the actual word names for these indices\n","    top_words = []\n","    for i in top_indices:\n","        top_words.append(feature_names[i])\n","\n","    # Join the top words into a single string\n","    top_words_string = \" \".join(top_words)\n","\n","    # Print and save\n","    print(\"Topic #{}:\".format(topic_index))\n","    print(top_words_string)\n","    topic_names.append(top_words_string)\n","topic_df = pd.DataFrame(W, columns=topic_names)\n","topic_df\n","\n","topic_df = pd.DataFrame(nmf_model.components_ ,columns=feature_names)\n","topic_df"]},{"cell_type":"markdown","metadata":{"id":"49df52gC8kGS"},"source":["This is the W matrix (document-topic distribution):\n","\n","\n","|     | today outside rain | watch season premiere |\n","|-----|--------------------|------------------------|\n","| 0   | 0.490981           | 0.000000               |\n","| 1   | 0.490981           | 0.000000               |\n","| 2   | 0.000000           | 0.840054               |\n","\n","And this is the H matrix (topic-word distribution):\n","\n","|     | outside   | premiere | rain     | season   | today    | watch    |\n","|-----|-----------|----------|----------|----------|----------|----------|\n","| 0   | 0.810582  | 0.000000 | 0.810582 | 0.000000 | 1.232936 | 0.000000 |\n","| 1   | 0.000000  | 0.687278 | 0.000000 | 0.687278 | 0.000000 | 0.687278 |"]},{"cell_type":"markdown","metadata":{"id":"IAFb6EFY8kGS"},"source":["\n","## 3. Analyzing Twitter Data\n","\n","Finally, we get to practice using the Twitter data!\n","### 3.1 What Social Media Accounts to Search?\n","\n","To identify social media accounts related to AI tools, we perform a Google search using the keyword \"AI marketing tools\". Below are the Search Engine Results Pages (also known as “SERPs” or “SERP”).\n","\n","The first few results are sponsored links, and one organic result points us to [15 Best AI Marketing Tools in 2023-2024](https://improvado.io/blog/best-ai-marketing-tools). Among the recommended AI tools, we are particularly interested in [Grammarly](https://twitter.com/Grammarly). Let's collect tweets generated by Grammarly's official account and examine which tweets get more likes.\n","\n","\n","> Grammarly is a cloud-based typing assistant. It reviews spelling, grammar, punctuation, clarity, engagement, and delivery mistakes in English texts, detects plagiarism, and suggests replacements for the identified errors. For a brief introduction to Grammarly, watch this [video](https://www.youtube.com/watch?v=zd64pGNLjVY).\n","\n","\n","### 3.2. Data Collection\n","Twitter has its API service. To simplify this data collection process, I built a little package.\n"]},{"cell_type":"code","source":["import os\n","from google.colab import userdata\n","os.environ[\"BEARER_TOKEN\"] =userdata.get('BEARER_TOKEN')\n","\n","from itom6219 import user_info, user_tweets, user_tweets_all\n","\n","user=user_info([\"grammarly\"])\n","tweets=user_tweets([\"grammarly\"], exclude_replies=True, exclude_retweets=True)\n",""],"metadata":{"id":"b6H6EovMlrYT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XaMirtjd8kGS"},"source":["To make sure that we are on the same page, let's use the same data:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zyi6VeUb8kGS"},"outputs":[],"source":["# We use pd.read_csv to read csv file\n","import pandas as pd\n","file_path = 'AI_tweets_2025.csv'\n","df = pd.read_csv(file_path)\n",""]},{"cell_type":"markdown","metadata":{"id":"ed5LgHdY8kGS"},"source":["> How to use pandas package to read csv file and convert it to dataframe?"]},{"cell_type":"markdown","metadata":{"id":"-YwCSJIe8kGT"},"source":["\n","##### Data Cleaning\n","\n","Data cleaning is a tedious process, and here we only clean the date time."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ajV2UYY8kGT"},"outputs":[],"source":["# Data cleaning\n","df['datetime']=pd.to_datetime(df['created_at'])"]},{"cell_type":"markdown","metadata":{"id":"HnoLbmov8kGT"},"source":["\n","#### Data Exploration\n","Log transformations allow us to see a more clear picture."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T8vfg07t8kGT"},"outputs":[],"source":["import plotly.express as px\n","import numpy as np\n","\n","df['log_view']=np.log1p(df['public_metrics.impression_count'])\n","\n","# create a line plot with Plotly Express\n","fig = px.line(df, x='datetime', y='log_view', title='Impression Over Time', template='plotly_white')\n","\n","# display the plot\n","fig.show()"]},{"cell_type":"markdown","metadata":{"id":"h9DIcJg38kGT"},"source":["> Can you visualize likes over time?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x9ER0zAN8kGT"},"outputs":[],"source":["## Add your code"]},{"cell_type":"markdown","metadata":{"id":"0bctEMY_8kGT"},"source":["One tweet stood out..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I4BDD4xO8kGT"},"outputs":[],"source":["df=df.sort_values(by='log_view', ascending=False)\n"]},{"cell_type":"markdown","metadata":{"id":"73ybRVjS8kGT"},"source":["To check the specific tweet, use this URL: https://x.com/Grammarly/status/1753062628711174651\n","It looks like that this is a promoted tweet."]},{"cell_type":"markdown","metadata":{"id":"YjhtFfMG8kGT"},"source":["## 3.3 Vectorization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ABpoFy1_8kGT"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","docs=df['text']\n","#Convert a collection of raw documents to a matrix of TF-IDF features.\n","tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n","tfidf = tfidf_vectorizer.fit_transform(docs)\n","\n","tfidf_df = pd.DataFrame.sparse.from_spmatrix(tfidf, columns=tfidf_vectorizer.get_feature_names_out())\n","tfidf_df"]},{"cell_type":"markdown","metadata":{"id":"5ajxrVi28kGU"},"source":["## 3.4 Topic Modeling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d4aELTBX8kGU"},"outputs":[],"source":["\n","# Apply NMF\n","from sklearn.decomposition import NMF\n","\n","nmf_model = NMF(n_components=7, random_state=0)\n","nmf_model.fit(tfidf)\n","W = nmf_model.fit_transform(tfidf)  # Document-topic matrix\n","\n","# Display topics\n","feature_names = tfidf_vectorizer.get_feature_names_out()\n","topic_names=[]\n","for topic_index in range(len(nmf_model.components_)):\n","    topic = nmf_model.components_[topic_index]\n","    # Get the indices of the top 3 words (largest values in the topic)\n","    sorted_indices = topic.argsort()  # sorts from smallest to largest\n","    top_indices = sorted_indices[-4:]  # get the last 3 (top 3 words)\n","    # Reverse to make it largest to smallest\n","    top_indices = top_indices[::-1]\n","    # Get the actual word names for these indices\n","    top_words = []\n","    for i in top_indices:\n","        top_words.append(feature_names[i])\n","    # Join the top words into a single string\n","    top_words_string = \" \".join(top_words)\n","    # Print and save\n","    print(\"Topic #{}:\".format(topic_index))\n","    print(top_words_string)\n","    topic_names.append(top_words_string)\n","\n","topic_df = pd.DataFrame(W, columns=topic_names)\n",""]},{"cell_type":"markdown","metadata":{"id":"OH6j8BBm8kGU"},"source":["\n","## 3.5 Linear Regression\n","\n","\n","Linear regression is one of the most commonly used techniques in data analysis. It helps us understand the relationship between one or more input variables (features) and an output variable (target). In the simplest case, it tries to draw a straight line that best fits the data.\n","\n","In our example, we want to understand:\n","\n","- How do the topics of Grammarly’s tweets influence the like-to-view?\n","- Which topics are more likely to lead to higher engagement (like-to-view)?\n","- Which topics seem to have less impact or even negative impact?\n","\n","Each tweet is represented as a set of topic weights (from NMF), and our target is the like ratio for that tweet.\n","\n","We’ll use the topic weights (<code>topic_df</code>) as features, and the like ratio as the outcome.\n","\n","### 3.5.1 Model\n","The model assumes a relationship of the form:\n","\n","$$\n","\\text{LikeRatio} = \\beta_0 + \\beta_1 \\cdot \\text{Topic}_1 + \\beta_2 \\cdot \\text{Topic}_2 + \\dots + \\beta_k \\cdot \\text{Topic}_k+\\epsilon\n","$$\n","\n","- $\\beta_0$ is the intercept.  \n","- $\\beta_1, \\beta_2, \\dots, \\beta_k$ are **coefficients** for each topic.  \n","- A **positive coefficient** ($\\beta_i > 0$) means the topic is associated with **more likes**.  \n","- A **negative coefficient** ($\\beta_i < 0$) means the topic is associated with **fewer likes**.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NrzUN-NE8kGU"},"outputs":[],"source":["print(df.columns.tolist())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LNZSpMVw8kGU"},"outputs":[],"source":["import pingouin as pg\n","\n","# Combine X and y into a single dataframe\n","df_model = topic_df.copy()\n","df_model['ratio'] = df['public_metrics.like_count'] / df['log_view']\n","\n","# Run linear regression\n","result = pg.linear_regression(df_model.drop(columns='ratio'), df_model['ratio'])\n","\n","# Display R-squared (only need the first row since it's repeated)\n","r2 = round(result['r2'].iloc[0], 3)\n","print(f\"\\nR-squared: {r2}\")\n","\n","# Round coef and pval to 3 decimal places\n","result[['names', 'coef', 'pval']] = result[['names', 'coef', 'pval']].round(3)\n","\n","# Display the rounded result\n","result[['names', 'coef', 'pval']]\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sPVJsN_U8kGU"},"source":["## Practice:\n","- How are the relationships between topics and views?\n","- Instead of using topics as input variables, try to use TF-IDF scores directly as input variables. Who did the R2 change?"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}